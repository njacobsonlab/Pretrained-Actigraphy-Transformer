{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGbNlrmYYCaF"
      },
      "source": [
        "# Baseline Test - 1D CNN\n",
        "Google TPU v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K9ZZ5rBYcul"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7sQUusVYCuE"
      },
      "outputs": [],
      "source": [
        "folder_root = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests\"\n",
        "folder_Data_2013 = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/Data_2013\"\n",
        "folder_1D_CNN = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/1D_CNN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzmZWbUnZa8N"
      },
      "source": [
        "# Imports and Connects to TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "x7Z7ocXsZfQ0"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C6SS4tLqZhZf"
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdG5nduEZhby"
      },
      "outputs": [],
      "source": [
        "# @title Importing\n",
        "\n",
        "# Packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.metrics import AUC\n",
        "\n",
        "# Tf\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import random\n",
        "\n",
        "# Import Layers\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import MaxPooling3D\n",
        "from keras.layers import GlobalAveragePooling1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKa08RfIZheM"
      },
      "outputs": [],
      "source": [
        "## SEEDS\n",
        "\n",
        "# Hard Code Random Seeds.\n",
        "r1 = 0\n",
        "r2 = 1\n",
        "\n",
        "# Set Random Seed\n",
        "random.seed(r1)\n",
        "tf.random.set_seed(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uyh0L8sDZhgV"
      },
      "outputs": [],
      "source": [
        "#@title Connect to TPU\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Connect to the TPU cluster or fall back to CPU/GPU\n",
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Tries to connect to the TPU\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "  devices = tf.config.list_logical_devices('TPU')\n",
        "  print('TPU devices:', devices)\n",
        "except ValueError:\n",
        "  print(\"Could not connect to TPU; using CPU/GPU strategy instead.\")\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Example computation using the strategy\n",
        "with strategy.scope():\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "  @tf.function\n",
        "  def matmul_fn(x, y):\n",
        "    return tf.matmul(x, y)\n",
        "\n",
        "  z = strategy.run(matmul_fn, args=(a, b))\n",
        "\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T98DF67i7-V"
      },
      "source": [
        "# Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhU8AAM3kaCE"
      },
      "source": [
        "## Define data mode (raw or smooth) and Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qy42PlETVBG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Please Fill out Parameters Below\n",
        "\"\"\"\n",
        "\n",
        "Smoothing = False\n",
        "\n",
        "Task = \"SSRI\"\n",
        "\n",
        "Tasks = [\"SSRI\", \"Benzodiazepine\", \"Sleep Strict\", \"Sleep Liberal\", \"Depression\"] # pick a task from Tasks and set the \"Task\" variable in the above line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgLq24j1zxRS"
      },
      "outputs": [],
      "source": [
        "if Smoothing:\n",
        "  mode = \"Smooth\"\n",
        "else:\n",
        "  mode = \"Raw\"\n",
        "\n",
        "\n",
        "if \"Depression\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 2800]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_Depression/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Strict\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_SleepDisorder_Strict/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Liberal\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_SleepDisorder_Liberal/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Benzos\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"SSRI\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_Taking_SSRI/{mode}/TestSize2000_set1\")\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Invalid condition\")\n",
        "\n",
        "print(f\"Current train sizes: {train_sizes}\")\n",
        "print(f\"Data located at {data_folder_location}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CQR61odDvav"
      },
      "outputs": [],
      "source": [
        "test_size = 2000 # fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDVG26nlDroF"
      },
      "outputs": [],
      "source": [
        "# first save the test sets\n",
        "X_test = np.load(os.path.join(data_folder_location, f'X_test_{test_size}.npy'))\n",
        "y_test = np.load(os.path.join(data_folder_location, f'y_test_{test_size}.npy'))\n",
        "print(\"successfully loaded X test and y test\")\n",
        "print(f\"Shape of X test: {X_test.shape}\")\n",
        "print(f\"Shape of y test: {y_test.shape}\")\n",
        "\n",
        "# standard scalar\n",
        "train_scalar = StandardScaler()\n",
        "train_scalar.fit(X_test)\n",
        "X_test = train_scalar.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0jrZLMkD_Zu"
      },
      "source": [
        "Then, load the train and validation datasets by saving them into a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcvyim1bEUJ-"
      },
      "outputs": [],
      "source": [
        "train_sets = {}\n",
        "val_sets = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5a5Uyfb1swC"
      },
      "outputs": [],
      "source": [
        "for size in train_sizes:\n",
        "  X_train = np.load(os.path.join(data_folder_location, f'X_train_{size}.npy'))\n",
        "  y_train = np.load(os.path.join(data_folder_location, f'y_train_{size}.npy'))\n",
        "  train_sets[size] = (X_train, y_train)\n",
        "\n",
        "  X_val = np.load(os.path.join(data_folder_location, f'X_val_{size}.npy'))\n",
        "  y_val = np.load(os.path.join(data_folder_location, f'y_val_{size}.npy'))\n",
        "  val_sets[size] = (X_val, y_val)\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Train set size: {len(train_sets)}\")\n",
        "print(f\"Val set size: {len(val_sets)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SJGid_-EfdN"
      },
      "source": [
        "### Sanity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8fAuKfEg1y"
      },
      "outputs": [],
      "source": [
        "for key, value in train_sets.items():\n",
        "  print(f\"For train size {key}: \")\n",
        "\n",
        "  # print the shapes of X train and y train\n",
        "  print(f\"X train shape: {value[0].shape}\")\n",
        "  print(f\"y train shape: {value[1].shape}\")\n",
        "\n",
        "  # also print the shapes of X val and y val\n",
        "  print(f\"X val shape: {val_sets[key][0].shape}\")\n",
        "  print(f\"y val shape: {val_sets[key][1].shape}\")\n",
        "\n",
        "  print(\"================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9dwiKMVj_T_"
      },
      "source": [
        "# The 1D CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFYUUU3iKiXZ"
      },
      "source": [
        "## Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWgcnq4qkBCv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model(X_train):\n",
        "  model = Sequential()\n",
        "\n",
        "  # add the first convolutional layer\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "  model.add(MaxPooling1D(pool_size=2)) # pooling\n",
        "  # add the second convolutional layer\n",
        "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2)) # pooling\n",
        "  model.add(GlobalAveragePooling1D()) # average pool\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.5)) # prevent overfitting\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IAJcBjKKm4L"
      },
      "source": [
        "### Since the 1D CNN Model depends on the size of X_train, we need to make a new model for every different X train size. Let's save them in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlJW7Y9vLSgM"
      },
      "outputs": [],
      "source": [
        "models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RurbMdalkWNw"
      },
      "outputs": [],
      "source": [
        "# Compile the model -----\n",
        "\n",
        "for train_size in train_sizes:\n",
        "  X_train, _ = train_sets[train_size]\n",
        "\n",
        "  print(f\"====== training model on size {train_size} ======\")\n",
        "  print(f\"X train shape: {X_train.shape}\")\n",
        "\n",
        "  with strategy.scope():\n",
        "    train_model = create_model(X_train=X_train)\n",
        "    train_model.compile(\n",
        "      # Metrics\n",
        "      loss= tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "      metrics= tf.keras.metrics.AUC(name='auc'),\n",
        "      # Optimizer\n",
        "      optimizer= tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.00001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False\n",
        "  ))\n",
        "\n",
        "  models[train_size] = train_model\n",
        "\n",
        "  print(\"MODEL SUMMARY: \", train_model.summary())\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i9zJ5RhgLhtN"
      },
      "outputs": [],
      "source": [
        "# inspect the model dictionary:\n",
        "for key, value in models.items():\n",
        "  print(f\"Model for train size {key}:\")\n",
        "  print(value.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKyUPAlMQLI"
      },
      "source": [
        "## Prepare model weights and callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrAz86BcHRzU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNwthwNgG-IP"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',    # monitor validation loss\n",
        "    factor=0.5,            # reduce rate by a factor of 0.5\n",
        "    patience=250,          # number of epochs with no improvement after which learning rate will be reduced\n",
        "    min_lr=1e-4,           # minimum learning rate that the reduction can reach\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# earlyStopping callback\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_auc',  # monitor validation AUC\n",
        "    mode='max',  # maximize AUC\n",
        "    patience=250,  # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,  # display messages when early stopping is triggered\n",
        "    restore_best_weights=True  # restore model weights from the epoch with the best value of the monitored quantity\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu3gFO_JHft3"
      },
      "source": [
        "## Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGZkgH1HNQMd"
      },
      "outputs": [],
      "source": [
        "# model weights saving path\n",
        "model_weight_path = os.path.join(folder_1D_CNN, f\"Model Weights/All_Meds{condition}/{mode}\")\n",
        "print(f\"current model weight path: {model_weight_path}\")\n",
        "if not os.path.exists(model_weight_path):\n",
        "  os.makedirs(model_weight_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LNtepf4Ng0w"
      },
      "outputs": [],
      "source": [
        "epochs = 10000\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq2yrezgOQC7"
      },
      "outputs": [],
      "source": [
        "scores = {}\n",
        "scores[\"test\"] = {}\n",
        "scores[\"val\"] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCobSVNrOewC"
      },
      "outputs": [],
      "source": [
        "model_histories = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p8fC2uKWkZVS"
      },
      "outputs": [],
      "source": [
        "for size in train_sizes:\n",
        "\n",
        "    print(f\"\\nSIZE:{size}\")\n",
        "\n",
        "\n",
        "    # Load X_train and fit\n",
        "    X_train, y_train = train_sets[size]\n",
        "    train_scalar = StandardScaler()\n",
        "    train_scalar.fit(X_train)\n",
        "    X_train = train_scalar.transform(X_train)\n",
        "\n",
        "    # Load X_val and fit\n",
        "    X_val, y_val = val_sets[size]\n",
        "    val_scalar = StandardScaler()\n",
        "    val_scalar.fit(X_val)\n",
        "    X_val = val_scalar.transform(X_val)\n",
        "\n",
        "    print(\"loaded X train and X val\")\n",
        "\n",
        "    # Set Class Weights = Balance\n",
        "    class1 = sum(y_train)\n",
        "    total = len(y_train)\n",
        "    class0 = total-class1\n",
        "\n",
        "    class_weights = {0: (class1/total),\n",
        "                  1: ((class0/total))}\n",
        "\n",
        "    print(f\"class weights: {class_weights}\")\n",
        "\n",
        "    # get the corresponding model from the model dictionary\n",
        "    train_model = models[size]\n",
        "    print(\"model loaded\")\n",
        "\n",
        "    # Train model\n",
        "    history = train_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs= epochs, # Edit\n",
        "        batch_size= batch_size,\n",
        "        validation_data = (X_val, y_val),\n",
        "        shuffle=False,\n",
        "        class_weight=class_weights,\n",
        "        callbacks = [early_stopper, reduce_lr],\n",
        "        verbose = 2)\n",
        "\n",
        "    # save model history\n",
        "    model_histories[size] = history\n",
        "\n",
        "    # Save model\n",
        "    current_model_name = f\"1D_CNN_{size}.h5\"\n",
        "    current_model_weights_name = f\"1D_CNN_{size}_weights.h5\"\n",
        "    print(\"current model name: \", current_model_name)\n",
        "    train_model.save(os.path.join(model_weight_path, current_model_name))\n",
        "    train_model.save_weights(os.path.join(model_weight_path, current_model_weights_name))\n",
        "    print(\"model and weights saved\")\n",
        "\n",
        "    # Test model\n",
        "    test_scores = train_model.evaluate(X_test, y_test, batch_size=64) # Test Set\n",
        "    scores[\"test\"][size] = test_scores[1]\n",
        "    print(\"Test AUC:\", test_scores[1])\n",
        "\n",
        "    val_scores = train_model.evaluate(X_val, y_val, batch_size=64) # Val Set\n",
        "\n",
        "    scores[\"val\"][size] = val_scores[1]\n",
        "    print(\"Val AUC:\", val_scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwYg64VYVBPd"
      },
      "outputs": [],
      "source": [
        "# let's look at the scores\n",
        "for key, value in scores.items():\n",
        "  print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nie3v8ZgPXfp"
      },
      "outputs": [],
      "source": [
        "# save all results in a .txt\n",
        "print(\"\\n\\n\")\n",
        "print(scores)\n",
        "\n",
        "results_path = os.path.join(folder_1D_CNN, f\"results{condition}_{mode}.txt\")\n",
        "\n",
        "try:\n",
        "    file_to_write = open(results_path, 'wt')\n",
        "    file_to_write.write(str(scores))\n",
        "    file_to_write.close()\n",
        "    print(\"Successfully wrote to file\")\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPiJtce0H9wv"
      },
      "source": [
        "# Inspect the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoyR9537WmuP"
      },
      "outputs": [],
      "source": [
        "model_histories_plot_path = os.path.join(folder_1D_CNN, f\"History Plots/All_Meds{condition}/{mode}\")\n",
        "# make the directory if it doesn't exist\n",
        "if not os.path.exists(model_histories_plot_path):\n",
        "  os.makedirs(model_histories_plot_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVeDvYSzXDIw"
      },
      "outputs": [],
      "source": [
        "#inspect model histories\n",
        "for key, value in model_histories.items():\n",
        "  print(f\"Model history for train size {key}:\")\n",
        "  print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH9ZbEDoB5qr"
      },
      "outputs": [],
      "source": [
        "# Save each plot individually\n",
        "for size, history in model_histories.items():\n",
        "    print(f\"Currently plotting graph with size {size}\")\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.title(f\"AUC over Epochs, Train Size {size}\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "    plt.plot(history.history['auc'], label='Train AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot\n",
        "    plot_path = os.path.join(model_histories_plot_path, f\"AUC_Over_Epochs_TrainSize{size}.png\")\n",
        "    plt.savefig(plot_path)  # Saves as PNG\n",
        "    print(f\"Saved figure at {plot_path}\")\n",
        "    plt.close()  # Close the plot to free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErnLQ_KuXAVB"
      },
      "outputs": [],
      "source": [
        "# save as one integrated plot\n",
        "\n",
        "num_models = len(model_histories)\n",
        "cols = 3  # number of columns in subplot grid\n",
        "rows = (num_models + cols - 1) // cols  # calculate required number of rows\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14 * cols, 5 * rows))\n",
        "fig.suptitle(\"AUC over Epochs by Train Size\", fontsize = 20)\n",
        "\n",
        "for idx, (size, history) in enumerate(model_histories.items()):\n",
        "    ax = axes[idx // cols, idx % cols]\n",
        "    ax.set_title(f\"Train Size {size}\")\n",
        "    ax.set_xlabel(\"Epochs\")\n",
        "    ax.set_ylabel(\"AUC\")\n",
        "    ax.plot(history.history['val_auc'], label='Validation AUC')\n",
        "    ax.plot(history.history['auc'], label='Train AUC')\n",
        "    ax.legend()\n",
        "\n",
        "# adjust layout to prevent overlap\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "# save the combined plot\n",
        "plot_path = os.path.join(model_histories_plot_path, \"Combined_AUC_Over_Epochs.png\")\n",
        "plt.savefig(plot_path)\n",
        "print(f\"Saved combined figure at {plot_path}\")\n",
        "plt.close()  # close the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EULUfNFYWy1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}