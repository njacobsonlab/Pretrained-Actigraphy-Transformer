{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DLnvDDAvZcg"
      },
      "source": [
        "# Baseline Test - ConvLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luetswgMv_8y"
      },
      "source": [
        "Trained with Google TPU v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpupLYYoxvNH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgz3qjavw6xX"
      },
      "outputs": [],
      "source": [
        "# write where you want to save all your files\n",
        "folder_root = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests\"\n",
        "folder_Data_2013 = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/Data_2013\"\n",
        "folder_ConvLSTM = \"/content/drive/MyDrive/ActigraphyTransformer/A-NEW/Baseline Tests/ConvLSTM\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSdGffCewcSF"
      },
      "source": [
        "# Imports and **Connect To TPU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSeNwoDSEHXt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gs2bFVn5ZMb"
      },
      "outputs": [],
      "source": [
        "# @title Importing\n",
        "\n",
        "# Packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.metrics import AUC\n",
        "\n",
        "# Tf\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import random\n",
        "\n",
        "# Import Layers\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import MaxPooling3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afsH2qmUxGQi"
      },
      "outputs": [],
      "source": [
        "## SEEDS\n",
        "\n",
        "# Hard Code Random Seeds.\n",
        "r1 = 0\n",
        "r2 = 1\n",
        "\n",
        "# Set Random Seed\n",
        "random.seed(r1)\n",
        "tf.random.set_seed(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4rQXYZcwz73",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Connect to TPU\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Connect to the TPU cluster or fall back to CPU/GPU\n",
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Tries to connect to the TPU\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "  devices = tf.config.list_logical_devices('TPU')\n",
        "  print('TPU devices:', devices)\n",
        "except ValueError:\n",
        "  print(\"Could not connect to TPU; using CPU/GPU strategy instead.\")\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Example computation using the strategy\n",
        "with strategy.scope():\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "  @tf.function\n",
        "  def matmul_fn(x, y):\n",
        "    return tf.matmul(x, y)\n",
        "\n",
        "  z = strategy.run(matmul_fn, args=(a, b))\n",
        "\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye8xyruzvf68"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Please Fill out Parameters Below\n",
        "\"\"\"\n",
        "\n",
        "Smoothing = False\n",
        "\n",
        "Task = \"SSRI\"\n",
        "\n",
        "Tasks = [\"SSRI\", \"Benzodiazepine\", \"Sleep Strict\", \"Sleep Liberal\", \"Depression\"] # pick a task from Tasks and set the \"Task\" variable in the above line"
      ],
      "metadata": {
        "id": "9ipXbzcRdJH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if Smoothing:\n",
        "  mode = \"Smooth\"\n",
        "else:\n",
        "  mode = \"Raw\"\n",
        "\n",
        "\n",
        "if \"Depression\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 2800]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_Depression/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Strict\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_SleepDisorder_Strict/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Liberal\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 3429]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_SleepDisorder_Liberal/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"Benzos\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds/{mode}/TestSize2000_set1\")\n",
        "\n",
        "elif \"SSRI\" in condition:\n",
        "  train_sizes = [100, 250, 500, 1000, 2500, 5769]\n",
        "  data_folder_location = os.path.join(folder_Data_2013, f\"All_Meds_Taking_SSRI/{mode}/TestSize2000_set1\")\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Invalid condition\")\n",
        "\n",
        "print(f\"Current train sizes: {train_sizes}\")\n",
        "print(f\"Data located at {data_folder_location}\")"
      ],
      "metadata": {
        "id": "ZhCv9I8wnrxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 2000 # fixed"
      ],
      "metadata": {
        "id": "yLKwPPVLddSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first save the test sets\n",
        "X_test = np.load(os.path.join(data_folder_location, f'X_test_{test_size}.npy'))\n",
        "y_test = np.load(os.path.join(data_folder_location, f'y_test_{test_size}.npy'))\n",
        "\n",
        "# standard scalar on X test\n",
        "train_scalar = StandardScaler()\n",
        "train_scalar.fit(X_test)\n",
        "X_test = train_scalar.transform(X_test)\n",
        "\n",
        "\n",
        "n_participants_test = X_test.shape[0]\n",
        "n_steps, n_length, n_width = 7, 24, 60\n",
        "n_features = 1\n",
        "X_test = X_test.reshape((X_test.shape[0], n_steps, n_length, n_width, n_features))\n",
        "\n",
        "print(\"successfully loaded X test and y test\")\n",
        "print(f\"Shape of X test: {X_test.shape}\")\n",
        "print(f\"Shape of y test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "eDGqy1eCdexA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, load the train and validation datasets by saving them into a dictionary"
      ],
      "metadata": {
        "id": "TfcwVnOZdkdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sets = {}\n",
        "val_sets = {}"
      ],
      "metadata": {
        "id": "5aEDkEHzdjh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for size in train_sizes:\n",
        "\n",
        "  # X train, X val original\n",
        "  X_train = np.load(os.path.join(data_folder_location, f'X_train_{size}.npy'))\n",
        "  X_val = np.load(os.path.join(data_folder_location, f'X_val_{size}.npy'))\n",
        "\n",
        "  # apply standard scalar\n",
        "  train_scalar = StandardScaler()\n",
        "  train_scalar.fit(X_train)\n",
        "  X_train = train_scalar.transform(X_train)\n",
        "\n",
        "  # Reshape Train, Validation and Test\n",
        "  n_participants_train = X_train.shape[0]\n",
        "  n_participants_val = X_val.shape[0]\n",
        "  n_timesteps = X_train.shape[1]\n",
        "  n_features = 1\n",
        "  X_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_width, n_features))\n",
        "  y_train = np.load(os.path.join(data_folder_location, f'y_train_{size}.npy'))\n",
        "\n",
        "  # X val\n",
        "  # apply standard scalar\n",
        "  val_scalar = StandardScaler()\n",
        "  val_scalar.fit(X_val)\n",
        "  X_val = val_scalar.transform(X_val)\n",
        "  X_val = X_val.reshape((X_val.shape[0], n_steps, n_length, n_width, n_features))\n",
        "\n",
        "  y_val = np.load(os.path.join(data_folder_location, f'y_val_{size}.npy'))\n",
        "\n",
        "  train_sets[size] = (X_train, y_train)\n",
        "  val_sets[size] = (X_val, y_val)\n",
        "\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Train set size: {len(train_sets)}\")\n",
        "print(f\"Val set size: {len(val_sets)}\")"
      ],
      "metadata": {
        "id": "YVn-5Gy4dnis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Checks"
      ],
      "metadata": {
        "id": "_yvvLm8FdpRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in train_sets.items():\n",
        "  print(f\"For train size {key}: \")\n",
        "\n",
        "  # print the shapes of X train and y train\n",
        "  print(f\"X train shape: {value[0].shape}\")\n",
        "  print(f\"y train shape: {value[1].shape}\")\n",
        "\n",
        "  # also print the shapes of X val and y val\n",
        "  print(f\"X val shape: {val_sets[key][0].shape}\")\n",
        "  print(f\"y val shape: {val_sets[key][1].shape}\")\n",
        "\n",
        "  print(\"================================\")"
      ],
      "metadata": {
        "id": "ryxt1DjpdrM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj_89bc4yqqi"
      },
      "source": [
        "# MODELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvoWSO8WMGn8"
      },
      "outputs": [],
      "source": [
        "# Model Structure\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  # conv Layers\n",
        "  model.add(ConvLSTM2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=(n_steps, n_length, n_width, n_features) ,return_sequences=True) )\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "  model.add(ConvLSTM2D(filters=64, kernel_size=(2, 2), padding='valid', return_sequences= False))\n",
        "\n",
        "  # feed forward Layers\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid')) #Sigmoid b/c our outcome is binary.\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling"
      ],
      "metadata": {
        "id": "6hUHE4Nze9Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model -----\n",
        "with strategy.scope():\n",
        "  train_model = create_model()\n",
        "  train_model.compile(\n",
        "    # Metrics\n",
        "    loss= tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics= tf.keras.metrics.AUC(name='auc'),\n",
        "    # Optimizer\n",
        "    optimizer= tf.keras.optimizers.Adam(\n",
        "      learning_rate=0.00001,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-07,\n",
        "      amsgrad=False\n",
        "))\n",
        "\n",
        "# save model weights\n",
        "train_model.save_weights('original_model_weights.h5')\n",
        "\n",
        "train_model.summary()"
      ],
      "metadata": {
        "id": "E_7oW1fHe-U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b199IkGy0_C"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ],
      "metadata": {
        "id": "tuTWUBdVfRL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mechanisms\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',    # Monitor validation loss\n",
        "    factor=0.5,            # Reduce rate by a factor of 0.5\n",
        "    patience=250,          # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    min_lr=1e-4,           # Minimum learning rate that the reduction can reach\n",
        "    verbose=1              # Print messages when reducing the learning rate\n",
        ")\n",
        "\n",
        "# earlyStopping callback\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_auc',  # monitor validation AUC\n",
        "    mode='max',  # maximize AUC\n",
        "    patience=250,  # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,  # display messages when early stopping is triggered\n",
        "    restore_best_weights=True  # restore model weights from the epoch with the best value of the monitored quantity\n",
        ")"
      ],
      "metadata": {
        "id": "UO7Uitf4fM1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set up weight folder path"
      ],
      "metadata": {
        "id": "bKUA0aX2fYlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model weights saving path\n",
        "model_weight_path = os.path.join(folder_ConvLSTM, f\"Model Weights/All_Meds{condition}/{mode}\")\n",
        "if not os.path.exists(model_weight_path):\n",
        "  os.makedirs(model_weight_path)\n",
        "print(f\"current model weight path: {model_weight_path}\")"
      ],
      "metadata": {
        "id": "b8_GLFmofbD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "hmP1MR6bfwpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "VbKQvR_ffyV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {}\n",
        "scores[\"test\"] = {}\n",
        "scores[\"val\"] = {}"
      ],
      "metadata": {
        "id": "rSQaWel-fpoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_histories = {}"
      ],
      "metadata": {
        "id": "VvCfk0gAfreU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGYMFGfwzCT4"
      },
      "outputs": [],
      "source": [
        "for size in train_sizes:\n",
        "\n",
        "    print(f\"\\nSIZE:{size}\")\n",
        "\n",
        "    # Load X_train and fit\n",
        "    X_train, y_train = train_sets[size]\n",
        "\n",
        "    # Load X_val and fit\n",
        "    X_val, y_val = val_sets[size]\n",
        "\n",
        "    print(f\"X Train size: {X_train.shape}\")\n",
        "    print(f\"X Val size: {X_val.shape}\")\n",
        "    print(f\"Y Train size: {y_train.shape}\")\n",
        "    print(f\"Y Val size: {y_val.shape}\")\n",
        "\n",
        "    print(\"loaded X train and X val\")\n",
        "\n",
        "    # Set Class Weights = Balance\n",
        "    class1 = sum(y_train)\n",
        "    total = len(y_train)\n",
        "    class0 = total-class1\n",
        "\n",
        "    class_weights = {0: (class1/total),\n",
        "                  1: ((class0/total))}\n",
        "\n",
        "    print(f\"class weights: {class_weights}\")\n",
        "\n",
        "    # reset model weights\n",
        "    train_model.load_weights('original_model_weights.h5')\n",
        "\n",
        "    # get the corresponding model from the model dictionary\n",
        "\n",
        "    print(\"model loaded\")\n",
        "\n",
        "    # Train model\n",
        "    history = train_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs= epochs, # Edit\n",
        "        batch_size= batch_size,\n",
        "        validation_data = (X_val, y_val),\n",
        "        shuffle=False,\n",
        "        class_weight=class_weights,\n",
        "        callbacks = [early_stopper, reduce_lr],\n",
        "        verbose = 2)\n",
        "\n",
        "    # save model history\n",
        "    model_histories[size] = history\n",
        "\n",
        "    # Save model\n",
        "    current_model_name = f\"ConvLSTM_{size}.h5\"\n",
        "    current_model_weights_name = f\"ConvLSTM__{size}_weights.h5\"\n",
        "    print(\"current model name: \", current_model_name)\n",
        "    train_model.save(os.path.join(model_weight_path, current_model_name))\n",
        "    train_model.save_weights(os.path.join(model_weight_path, current_model_weights_name))\n",
        "    print(\"model and weights saved\")\n",
        "\n",
        "    # Test model\n",
        "    test_scores = train_model.evaluate(X_test, y_test, batch_size=64) # Test Set\n",
        "    scores[\"test\"][size] = test_scores[1]\n",
        "    print(\"Test AUC:\", test_scores[1])\n",
        "\n",
        "    val_scores = train_model.evaluate(X_val, y_val, batch_size=64) # Val Set\n",
        "\n",
        "    scores[\"val\"][size] = val_scores[1]\n",
        "    print(\"Val AUC:\", val_scores[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvhl1OgQzp3f"
      },
      "outputs": [],
      "source": [
        "# let's look at the scores\n",
        "for key, value in scores.items():\n",
        "  print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all results in a .txt\n",
        "print(\"\\n\\n\")\n",
        "print(scores)\n",
        "\n",
        "results_path = os.path.join(folder_ConvLSTM, f\"results{condition}_{mode}.txt\")\n",
        "\n",
        "try:\n",
        "    file_to_write = open(results_path, 'wt')\n",
        "    file_to_write.write(str(scores))\n",
        "    file_to_write.close()\n",
        "    print(\"Successfully wrote to file\")\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ],
      "metadata": {
        "id": "KuehQapZhu9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWaVmsdLz_Rf"
      },
      "source": [
        "# Model Introspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9iEdU1Y0SDH"
      },
      "source": [
        "AUC OVER EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_histories_plot_path = os.path.join(folder_ConvLSTM, f\"History Plots/All_Meds{condition}/{mode}\")\n",
        "if not os.path.exists(model_histories_plot_path):\n",
        "  os.makedirs(model_histories_plot_path)\n",
        "print(f\"current model weight path: {model_histories_plot_path}\")"
      ],
      "metadata": {
        "id": "16kjLJ0iAuxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect model histories\n",
        "for key, value in model_histories.items():\n",
        "  print(f\"Model history for train size {key}:\")\n",
        "  print(value)"
      ],
      "metadata": {
        "id": "ldok24FJAvs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each plot individually\n",
        "for size, history in model_histories.items():\n",
        "    print(f\"Currently plotting graph with size {size}\")\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.title(f\"AUC over Epochs, Train Size {size}\")\n",
        "    plt.ylabel(\"AUC\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "    plt.plot(history.history['auc'], label='Train AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot\n",
        "    plot_path = os.path.join(model_histories_plot_path, f\"AUC_Over_Epochs_TrainSize{size}.png\")\n",
        "    plt.savefig(plot_path)  # Saves as PNG\n",
        "    print(f\"Saved figure at {plot_path}\")\n",
        "    plt.close()  # Close the plot to free up memory"
      ],
      "metadata": {
        "id": "fhMvBnFYAzae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as one big plot\n",
        "\n",
        "num_models = len(model_histories)\n",
        "cols = 3  # Number of columns in subplot grid\n",
        "rows = (num_models + cols - 1) // cols  # Calculate required number of rows\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(14 * cols, 5 * rows))\n",
        "fig.suptitle(\"AUC over Epochs by Train Size\", fontsize = 20)\n",
        "\n",
        "for idx, (size, history) in enumerate(model_histories.items()):\n",
        "    ax = axes[idx // cols, idx % cols]\n",
        "    ax.set_title(f\"Train Size {size}\")\n",
        "    ax.set_xlabel(\"Epochs\")\n",
        "    ax.set_ylabel(\"AUC\")\n",
        "    ax.plot(history.history['val_auc'], label='Validation AUC')\n",
        "    ax.plot(history.history['auc'], label='Train AUC')\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "# Save the combined plot\n",
        "plot_path = os.path.join(model_histories_plot_path, \"Combined_AUC_Over_Epochs.png\")\n",
        "plt.savefig(plot_path)\n",
        "print(f\"Saved combined figure at {plot_path}\")\n",
        "plt.close()  # Close the plot"
      ],
      "metadata": {
        "id": "vIiSNlRXA1E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_OwHOkzA2w2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}