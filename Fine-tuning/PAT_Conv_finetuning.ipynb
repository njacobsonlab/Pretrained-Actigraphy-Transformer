{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DLnvDDAvZcg"
      },
      "source": [
        "# PAT Conv Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luetswgMv_8y"
      },
      "source": [
        "Trained with Google TPU v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLBWYOLfN7Vt"
      },
      "source": [
        "#Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpupLYYoxvNH",
        "outputId": "e8c9d006-d468-4906-c3a2-fe6d453736bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gs2bFVn5ZMb",
        "outputId": "0d190d64-f9fa-4966-fb09-606807df169c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.1.4)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2024.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.8.3 fastparquet-2024.5.0\n"
          ]
        }
      ],
      "source": [
        "# @title Importing\n",
        "\n",
        "#Installs\n",
        "!pip install pyarrow fastparquet\n",
        "\n",
        "# Packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.metrics import AUC\n",
        "\n",
        "# Tf\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afsH2qmUxGQi"
      },
      "outputs": [],
      "source": [
        "#@title Random Seeds\n",
        "import random\n",
        "## SEEDS\n",
        "\n",
        "# Hard Code Random Seeds.\n",
        "r1 = 0\n",
        "r2 = 1\n",
        "\n",
        "# Set Random Seed\n",
        "random.seed(r1)\n",
        "tf.random.set_seed(r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N4rQXYZcwz73",
        "outputId": "c1b33499-8844-4425-8bdf-d04d84dec798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "TPU devices: [LogicalDevice(name='/device:TPU:0', device_type='TPU'), LogicalDevice(name='/device:TPU:1', device_type='TPU'), LogicalDevice(name='/device:TPU:2', device_type='TPU'), LogicalDevice(name='/device:TPU:3', device_type='TPU'), LogicalDevice(name='/device:TPU:4', device_type='TPU'), LogicalDevice(name='/device:TPU:5', device_type='TPU'), LogicalDevice(name='/device:TPU:6', device_type='TPU'), LogicalDevice(name='/device:TPU:7', device_type='TPU')]\n",
            "PerReplica:{\n",
            "  0: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  1: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  2: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  3: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  4: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  5: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  6: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32),\n",
            "  7: tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to TPU\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Connect to the TPU cluster or fall back to CPU/GPU\n",
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Tries to connect to the TPU\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(resolver)\n",
        "  devices = tf.config.list_logical_devices('TPU')\n",
        "  print('TPU devices:', devices)\n",
        "except ValueError:\n",
        "  print(\"Could not connect to TPU; using CPU/GPU strategy instead.\")\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "\n",
        "# Example computation using the strategy\n",
        "with strategy.scope():\n",
        "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "  @tf.function\n",
        "  def matmul_fn(x, y):\n",
        "    return tf.matmul(x, y)\n",
        "\n",
        "  z = strategy.run(matmul_fn, args=(a, b))\n",
        "\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6FAjx67HjgF"
      },
      "source": [
        "# Hyperparameters & Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hgz3qjavw6xX"
      },
      "outputs": [],
      "source": [
        "# write where you want to save all your files and retrieve encoder\n",
        "root = \"/content/drive/MyDrive/Extra Curricular /ActigraphyTransformer/A-NEW/PAT Experiments /PAT Conv Finetuning/Models\"\n",
        "encoder_root = \"/content/drive/MyDrive/Extra Curricular /ActigraphyTransformer/A-NEW/PAT Experiments /PAT Conv Pretraining/Encoders\"\n",
        "results_root = \"/content/drive/MyDrive/Extra Curricular /ActigraphyTransformer/A-NEW/PAT Experiments /PAT Conv Finetuning/Results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM4cghQbLMn9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Please Fill out Parameters Below\n",
        "\"\"\"\n",
        "## Model size\n",
        "# eg. [\"small\", \"medium\", \"large\", \"huge\"]\n",
        "size = \"large\"\n",
        "\n",
        "## Mask ratio\n",
        "# eg. [.25, .50, .75]\n",
        "mask_ratio = 0.90\n",
        "\n",
        "## Smoothing\n",
        "# eg. [True, False]\n",
        "smoothing = False\n",
        "\n",
        "## Loss Function\n",
        "# eg. [True, False], meaning MSE on only the masked portion or everything in the reconstruction\n",
        "mse_only_masked = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN-fXaQZ_zA-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Finetuning Specific Settings\n",
        "\"\"\"\n",
        "\n",
        "## Finetuning Styles\n",
        "# Add more if needed\n",
        "\n",
        "finetuning_styles = [\"full\", \"linear_probe\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ0wX2lh9I1o",
        "outputId": "fbd765fb-8f5d-4a54-a883-e6cde4a221dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/conv_encoder_large_90_unsmoothed_mse_all.h5\n"
          ]
        }
      ],
      "source": [
        "# Encoder naming\n",
        "mask_name = int(mask_ratio*100)\n",
        "\n",
        "encoder_name = f\"/conv_encoder_{size}_{mask_name}\"\n",
        "\n",
        "if smoothing == True:\n",
        "  encoder_name = f\"{encoder_name}_smoothed\"\n",
        "else:\n",
        "  encoder_name = f\"{encoder_name}_unsmoothed\"\n",
        "\n",
        "if mse_only_masked == True:\n",
        "  encoder_name = f\"{encoder_name}_mse_only_masked.h5\"\n",
        "else:\n",
        "  encoder_name = f\"{encoder_name}_mse_all.h5\"\n",
        "\n",
        "print(encoder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lr8FC1YaoDB",
        "outputId": "bda60957-aaa2-4a70-c4ab-053376decf5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/conv_AcT_large_90_unsmoothed_mse_all\n"
          ]
        }
      ],
      "source": [
        "# Start of finetuning name\n",
        "ft_name = f\"/conv_AcT_{size}_{mask_name}\"\n",
        "\n",
        "if smoothing == True:\n",
        "  ft_name = f\"{ft_name}_smoothed\"\n",
        "else:\n",
        "  ft_name = f\"{ft_name}_unsmoothed\"\n",
        "\n",
        "if mse_only_masked == True:\n",
        "  ft_name = f\"{ft_name}_mse_only_masked\"\n",
        "else:\n",
        "  ft_name = f\"{ft_name}_mse_all\"\n",
        "\n",
        "print(ft_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B_fdXDFfWZK"
      },
      "outputs": [],
      "source": [
        "encoder_path = encoder_root + encoder_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ug9_cnGL0OT"
      },
      "source": [
        "# Hyperparameter Additional Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0rxwgR_HkrB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model Size\n",
        "\"\"\"\n",
        "## Model Size\n",
        "if size == \"small\":\n",
        "\n",
        "  patch_size = 18\n",
        "  embed_dim = 96\n",
        "  # encoder\n",
        "  encoder_num_heads = 6\n",
        "  encoder_ff_dim = 256\n",
        "  encoder_num_layers = 1\n",
        "  encoder_rate = 0.1\n",
        "  # decoder\n",
        "  decoder_num_heads = 6\n",
        "  decoder_ff_dim = 256\n",
        "  decoder_num_layers = 1\n",
        "  decoder_rate = 0.1\n",
        "\n",
        "if size == \"medium\":\n",
        "\n",
        "  patch_size = 18\n",
        "  embed_dim = 96\n",
        "  # encoder\n",
        "  encoder_num_heads = 12\n",
        "  encoder_ff_dim = 256\n",
        "  encoder_num_layers = 2\n",
        "  encoder_rate = 0.1\n",
        "  # decoder\n",
        "  decoder_num_heads = 12\n",
        "  decoder_ff_dim = 256\n",
        "  decoder_num_layers = 1\n",
        "  decoder_rate = 0.1\n",
        "\n",
        "if size == \"large\":\n",
        "\n",
        "  patch_size = 9\n",
        "  embed_dim = 96\n",
        "  # encoder\n",
        "  encoder_num_heads = 12\n",
        "  encoder_ff_dim = 256\n",
        "  encoder_num_layers = 4\n",
        "  encoder_rate = 0.1\n",
        "  # decoder\n",
        "  decoder_num_heads = 12\n",
        "  decoder_ff_dim = 256\n",
        "  decoder_num_layers = 1\n",
        "  decoder_rate = 0.1\n",
        "\n",
        "if size == \"huge\":\n",
        "\n",
        "  patch_size = 5\n",
        "  embed_dim = 96\n",
        "  # encoder\n",
        "  encoder_num_heads = 12\n",
        "  encoder_ff_dim = 256\n",
        "  encoder_num_layers = 8\n",
        "  encoder_rate = 0.1\n",
        "  # decoder\n",
        "  decoder_num_heads = 12\n",
        "  decoder_ff_dim = 256\n",
        "  decoder_num_layers = 1\n",
        "  decoder_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyWY0OzPKdxQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "For Finetuning\n",
        "\"\"\"\n",
        "\n",
        "## Model Size\n",
        "if size == \"small\":\n",
        "\n",
        "  learning_rate = 0.00001\n",
        "  early_stopping_patience = 250\n",
        "\n",
        "  reduce_lr_patience = 75\n",
        "  min_lr = 1e-6\n",
        "\n",
        "if size == \"medium\":\n",
        "\n",
        "  learning_rate = 0.00001\n",
        "  early_stopping_patience = 250\n",
        "\n",
        "  reduce_lr_patience = 75\n",
        "  min_lr = 1e-6\n",
        "\n",
        "if size == \"large\":\n",
        "\n",
        "  learning_rate = 0.000001\n",
        "  early_stopping_patience = 250\n",
        "\n",
        "  reduce_lr_patience = 75\n",
        "  min_lr = 1e-7\n",
        "\n",
        "\n",
        "if size == \"huge\":\n",
        "\n",
        "  learning_rate = 0.0000005\n",
        "  early_stopping_patience = 250\n",
        "\n",
        "  reduce_lr_patience = 100\n",
        "  min_lr = 1e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lTk0F6CcxHI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Smoothing\n",
        "\"\"\"\n",
        "if smoothing == True:\n",
        "  data_folder_location = \"/content/drive/MyDrive/Extra Curricular /ActigraphyTransformer/A-NEW/Baseline Tests/Data_2013/All_Meds/Smooth/TestSize2000_set1\"\n",
        "\n",
        "else:\n",
        "  data_folder_location = \"/content/drive/MyDrive/Extra Curricular /ActigraphyTransformer/A-NEW/Baseline Tests/Data_2013/All_Meds/Raw/TestSize2000_set1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye8xyruzvf68"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1OS8svAAbFY"
      },
      "outputs": [],
      "source": [
        "# Which sizes to look at\n",
        "train_sizes = [100, 250, 500, 1000, 2500, 5769] # for PAT hyperparameter tuning, we can test on less datasets\n",
        "test_size = 2000 # fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia0lQCYwAjqH"
      },
      "outputs": [],
      "source": [
        "# first save the test sets\n",
        "X_test = np.load(os.path.join(data_folder_location, f'X_test_{test_size}.npy'))\n",
        "y_test = np.load(os.path.join(data_folder_location, f'y_test_{test_size}.npy'))\n",
        "\n",
        "\n",
        "# Scale the test set\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_test)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EctG1GkmHjXf",
        "outputId": "711ba948-cb44-4749-c114-64d26039d17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 10080)\n",
            "(2000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omax0tlpeIOn"
      },
      "outputs": [],
      "source": [
        "train_sets = {}\n",
        "val_sets = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rshy1g88eJsx",
        "outputId": "86350e31-604e-4e2d-b628-ea34efceaa50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Train set size: 6\n",
            "Val set size: 6\n"
          ]
        }
      ],
      "source": [
        "for size in train_sizes:\n",
        "  X_train = np.load(os.path.join(data_folder_location, f'X_train_{size}.npy'))\n",
        "  y_train = np.load(os.path.join(data_folder_location, f'y_train_{size}.npy'))\n",
        "  train_sets[size] = (X_train, y_train)\n",
        "\n",
        "  X_val = np.load(os.path.join(data_folder_location, f'X_val_{size}.npy'))\n",
        "  y_val = np.load(os.path.join(data_folder_location, f'y_val_{size}.npy'))\n",
        "  val_sets[size] = (X_val, y_val)\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Train set size: {len(train_sets)}\")\n",
        "print(f\"Val set size: {len(val_sets)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIU90iu1eNYx",
        "outputId": "208ccb41-06e8-461c-aaf9-97299f750f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For train size 100: \n",
            "X train shape: (80, 10080)\n",
            "y train shape: (80, 1)\n",
            "X val shape: (20, 10080)\n",
            "y val shape: (20, 1)\n",
            "================================\n",
            "For train size 250: \n",
            "X train shape: (200, 10080)\n",
            "y train shape: (200, 1)\n",
            "X val shape: (50, 10080)\n",
            "y val shape: (50, 1)\n",
            "================================\n",
            "For train size 500: \n",
            "X train shape: (400, 10080)\n",
            "y train shape: (400, 1)\n",
            "X val shape: (100, 10080)\n",
            "y val shape: (100, 1)\n",
            "================================\n",
            "For train size 1000: \n",
            "X train shape: (800, 10080)\n",
            "y train shape: (800, 1)\n",
            "X val shape: (200, 10080)\n",
            "y val shape: (200, 1)\n",
            "================================\n",
            "For train size 2500: \n",
            "X train shape: (2000, 10080)\n",
            "y train shape: (2000, 1)\n",
            "X val shape: (500, 10080)\n",
            "y val shape: (500, 1)\n",
            "================================\n",
            "For train size 5769: \n",
            "X train shape: (4615, 10080)\n",
            "y train shape: (4615, 1)\n",
            "X val shape: (1154, 10080)\n",
            "y val shape: (1154, 1)\n",
            "================================\n"
          ]
        }
      ],
      "source": [
        "for key, value in train_sets.items():\n",
        "  print(f\"For train size {key}: \")\n",
        "\n",
        "  # print the shapes of X train and y train\n",
        "  print(f\"X train shape: {value[0].shape}\")\n",
        "  print(f\"y train shape: {value[1].shape}\")\n",
        "\n",
        "  # also print the shapes of X val and y val\n",
        "  print(f\"X val shape: {val_sets[key][0].shape}\")\n",
        "  print(f\"y val shape: {val_sets[key][1].shape}\")\n",
        "\n",
        "  print(\"================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02mit2U_t0y"
      },
      "source": [
        "# Wait for later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gT_O4t9zhBs"
      },
      "source": [
        "# LOAD PAT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu9PeUxXlFUT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model\n",
        "\n",
        "# Modified Transformer Block to output attention weights with explicit layer names (otherwise the same as the )\n",
        "def TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.1, name_prefix=\"encoder\"):\n",
        "    input_layer = layers.Input(shape=(None, embed_dim), name=f\"{name_prefix}_input\")\n",
        "    attention_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name=f\"{name_prefix}_attention\")\n",
        "    attention_output, attention_weights = attention_layer(input_layer, input_layer, return_attention_scores=True)\n",
        "    attention_output = layers.Dropout(rate, name=f\"{name_prefix}_dropout\")(attention_output)\n",
        "    out1 = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_norm1\")(input_layer + attention_output)\n",
        "    ff_output = layers.Dense(ff_dim, activation=\"relu\", name=f\"{name_prefix}_ff1\")(out1)\n",
        "    ff_output = layers.Dense(embed_dim, name=f\"{name_prefix}_ff2\")(ff_output)\n",
        "    ff_output = layers.Dropout(rate, name=f\"{name_prefix}_dropout2\")(ff_output)\n",
        "    final_output = layers.LayerNormalization(epsilon=1e-6, name=f\"{name_prefix}_norm2\")(out1 + ff_output)\n",
        "    return models.Model(inputs=input_layer, outputs=[final_output, attention_weights], name=f\"{name_prefix}_transformer\")\n",
        "\n",
        "# Sine/Cosine positional embeddings\n",
        "def get_positional_embeddings(num_patches, embed_dim):\n",
        "    position = tf.range(num_patches, dtype=tf.float32)[:, tf.newaxis]\n",
        "    div_term = tf.exp(tf.range(0, embed_dim, 2, dtype=tf.float32) * (-tf.math.log(10000.0) / embed_dim))\n",
        "    pos_embeddings = tf.concat([tf.sin(position * div_term), tf.cos(position * div_term)], axis=-1)\n",
        "    return pos_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqtjZtr4o1Fo"
      },
      "outputs": [],
      "source": [
        "# Function to load the encoder and build the fine-tuning model with consistent patching and positional embedding\n",
        "def create_finetuning_model(encoder_path=encoder_path, input_size=10080, patch_size=patch_size, embed_dim=embed_dim, return_attention=False):\n",
        "\n",
        "    # Load the saved encoder model\n",
        "    encoder_model = tf.keras.models.load_model(encoder_path, custom_objects={'TransformerBlock': TransformerBlock, 'get_positional_embeddings': get_positional_embeddings})\n",
        "\n",
        "    # Define new inputs for the fine-tuning model\n",
        "    inputs = layers.Input(shape=(input_size,), name=\"finetuning_inputs\")\n",
        "\n",
        "    # Get encoder outputs\n",
        "    encoder_outputs = encoder_model(inputs)\n",
        "    encoder_outputs, attention_weights = encoder_outputs[0], encoder_outputs[1:]\n",
        "\n",
        "    # Pass through a GlobalAveragePooling layer\n",
        "    x = layers.GlobalAveragePooling1D(name=\"global_avg_pool\")(encoder_outputs)\n",
        "    x = layers.Dropout(0.1, name=\"dropout\")(x)\n",
        "    x = layers.Dense(128, activation='relu', name=\"dense_128\")(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
        "\n",
        "    # Include attention weights in the final model outputs if requested\n",
        "    if return_attention:\n",
        "        outputs = [outputs] + attention_weights\n",
        "\n",
        "    # Create and return the fine-tuning model\n",
        "    finetuning_model = models.Model(inputs=inputs, outputs=outputs, name=\"finetuning_model\")\n",
        "    return finetuning_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vI4lmYRg8BE"
      },
      "source": [
        "## Compiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gBQPBmsgyYE",
        "outputId": "74dd6e19-183b-43f0-9798-5a11be77718a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"finetuning_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " finetuning_inputs (InputLa  [(None, 10080)]           0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " encoder_model (Functional)  [(None, 1120, 96),        1983616   \n",
            "                              (None, 12, 1120, 1120)             \n",
            "                             , (None, 12, 1120, 1120             \n",
            "                             ),                                  \n",
            "                              (None, 12, 1120, 1120)             \n",
            "                             , (None, 12, 1120, 1120             \n",
            "                             )]                                  \n",
            "                                                                 \n",
            " global_avg_pool (GlobalAve  (None, 96)                0         \n",
            " ragePooling1D)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 96)                0         \n",
            "                                                                 \n",
            " dense_128 (Dense)           (None, 128)               12416     \n",
            "                                                                 \n",
            " output (Dense)              (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1996161 (7.61 MB)\n",
            "Trainable params: 1996161 (7.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Compile the model -----\n",
        "with strategy.scope():\n",
        "  train_model = create_finetuning_model(return_attention=False)\n",
        "  train_model.compile(\n",
        "    # Metrics\n",
        "    loss= tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics= tf.keras.metrics.AUC(name='auc'),\n",
        "    # Optimizer\n",
        "    optimizer= tf.keras.optimizers.Adam(\n",
        "      learning_rate=learning_rate,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-07,\n",
        "      amsgrad=False\n",
        "))\n",
        "\n",
        "# Save the original model weights\n",
        "train_model.save_weights('original_model_weights.h5')\n",
        "\n",
        "train_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpOx9Fi4I6e-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N8DF9BclK1D-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# # reduce learning rate (Don't use this it's buggy)\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',    # Monitor validation loss\n",
        "#     factor=0.5,            # Reduce rate by a factor of 0.5\n",
        "#     patience=75,           # Number of epochs with no improvement after which learning rate will be reduced\n",
        "#     min_lr=1e-6,           # Minimum learning rate that the reduction can reach\n",
        "#     verbose=1              # Print messages when reducing the learning rate\n",
        "# )\n",
        "\n",
        "# earlyStopping callback\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_auc',  # monitor validation AUC\n",
        "    mode='max',  # maximize AUC\n",
        "    patience=early_stopping_patience,  # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,  # display messages when early stopping is triggered\n",
        "    restore_best_weights=True  # restore model weights from the epoch with the best value of the monitored quantity\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yk6GQYtH6oJv",
        "outputId": "51d8225e-a99b-4995-c92f-20018fa2576b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finetuning Style: full\n",
            "\n",
            "SIZE:100\n",
            " \n",
            "finetuning_inputs True\n",
            "encoder_model True\n",
            "global_avg_pool True\n",
            "dropout True\n",
            "dense_128 True\n",
            "output True\n",
            " \n",
            "Epoch 1/10000\n",
            "2/2 - 35s - loss: 0.0343 - auc: 0.4519 - val_loss: 0.8860 - val_auc: 0.2368 - 35s/epoch - 18s/step\n",
            "Epoch 2/10000\n",
            "2/2 - 1s - loss: 0.0323 - auc: 0.6859 - val_loss: 0.8824 - val_auc: 0.2632 - 813ms/epoch - 406ms/step\n",
            "Epoch 3/10000\n",
            "2/2 - 1s - loss: 0.0344 - auc: 0.5256 - val_loss: 0.8784 - val_auc: 0.2368 - 723ms/epoch - 361ms/step\n",
            "Epoch 4/10000\n",
            "2/2 - 1s - loss: 0.0337 - auc: 0.5865 - val_loss: 0.8747 - val_auc: 0.2632 - 719ms/epoch - 360ms/step\n",
            "Epoch 5/10000\n",
            "2/2 - 1s - loss: 0.0317 - auc: 0.7404 - val_loss: 0.8710 - val_auc: 0.2368 - 736ms/epoch - 368ms/step\n",
            "Epoch 6/10000\n",
            "2/2 - 1s - loss: 0.0337 - auc: 0.6186 - val_loss: 0.8670 - val_auc: 0.2632 - 730ms/epoch - 365ms/step\n",
            "Epoch 7/10000\n",
            "2/2 - 1s - loss: 0.0335 - auc: 0.6090 - val_loss: 0.8632 - val_auc: 0.2632 - 709ms/epoch - 355ms/step\n",
            "Epoch 8/10000\n",
            "2/2 - 1s - loss: 0.0341 - auc: 0.5769 - val_loss: 0.8593 - val_auc: 0.2632 - 1s/epoch - 527ms/step\n",
            "Epoch 9/10000\n",
            "2/2 - 1s - loss: 0.0349 - auc: 0.4519 - val_loss: 0.8553 - val_auc: 0.2632 - 742ms/epoch - 371ms/step\n",
            "Epoch 10/10000\n",
            "2/2 - 1s - loss: 0.0328 - auc: 0.6410 - val_loss: 0.8516 - val_auc: 0.2632 - 750ms/epoch - 375ms/step\n",
            "Epoch 11/10000\n",
            "2/2 - 1s - loss: 0.0327 - auc: 0.6538 - val_loss: 0.8481 - val_auc: 0.2632 - 729ms/epoch - 365ms/step\n",
            "Epoch 12/10000\n",
            "2/2 - 1s - loss: 0.0357 - auc: 0.3686 - val_loss: 0.8441 - val_auc: 0.2632 - 745ms/epoch - 372ms/step\n",
            "Epoch 13/10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-28d70d102414>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     history = train_model.fit(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Edit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1854\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m                         )\n\u001b[0;32m-> 1856\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1857\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotune_steps_per_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2285\u001b[0;31m             for (\n\u001b[0m\u001b[1;32m   2286\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m                                    True)\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m     worker_iterators = _create_iterators_per_worker(\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloned_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_create_iterators_per_worker\u001b[0;34m(worker_datasets, input_workers, options, canonicalize_devices)\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m       \u001b[0mworker_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_devices_for_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m       iterator = _SingleWorkerOwnedDatasetIterator(\n\u001b[0m\u001b[1;32m   1663\u001b[0m           \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworker_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m           \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, worker, devices, components, element_spec, options, canonicalize_devices)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m       super(_SingleWorkerOwnedDatasetIterator,\n\u001b[0;32m-> 1557\u001b[0;31m             self).__init__(dataset, worker, devices, self._options)\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_owned_multi_device_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, worker, devices, options)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_make_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1595\u001b[0m                        \"owned iterator.\")\n\u001b[1;32m   1596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_should_use_multi_device_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_owned_multi_device_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py\u001b[0m in \u001b[0;36m_create_owned_multi_device_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1586\u001b[0m             .experimental_per_replica_buffer_size)\n\u001b[1;32m   1587\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m         self._iterator = multi_device_iterator_ops.OwnedMultiDeviceIterator(\n\u001b[0m\u001b[1;32m   1589\u001b[0m             self._dataset, self._devices, source_device=host_device)\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, devices, max_buffer_size, prefetch_buffer_size, source_device, components, element_spec)\u001b[0m\n\u001b[1;32m    517\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           ds = _PerDeviceGenerator(\n\u001b[0m\u001b[1;32m    520\u001b[0m               \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_device_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shard_num, multi_device_iterator_resource, incarnation_id, source_device, element_spec, iterator_is_anonymous)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mfinalize_func_concrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finalize_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# TODO(b/124254153): Enable autograph once the overhead is low enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    270\u001b[0m           \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m       ):\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mfunc_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_graph_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         if (\n\u001b[1;32m    274\u001b[0m             \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \"\"\"\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   def __init__(self,\n\u001b[0m\u001b[1;32m    164\u001b[0m                \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "scores = {}\n",
        "scores[\"test\"] = {}\n",
        "scores[\"val\"] = {}\n",
        "\n",
        "for finetuning_style in finetuning_styles:\n",
        "  print(f\"\\nFinetuning Style: {finetuning_style}\")\n",
        "\n",
        "  scores[\"test\"][finetuning_style] = {}\n",
        "  scores[\"val\"][finetuning_style] = {}\n",
        "\n",
        "  for size in train_sizes:\n",
        "\n",
        "    print(f\"\\nSIZE:{size}\")\n",
        "\n",
        "\n",
        "    # Load X_train and fit\n",
        "    X_train, y_train = train_sets[size]\n",
        "    train_scalar = StandardScaler()\n",
        "    train_scalar.fit(X_train)\n",
        "    X_train = train_scalar.transform(X_train)\n",
        "\n",
        "    # Load X_val and fit\n",
        "    X_val, y_val = val_sets[size]\n",
        "    val_scalar = StandardScaler()\n",
        "    val_scalar.fit(X_val)\n",
        "    X_val = val_scalar.transform(X_val)\n",
        "\n",
        "    # Set Class Weights = Balance\n",
        "    class1 = sum(y_train)\n",
        "    total = len(y_train)\n",
        "    class0 = total-class1\n",
        "\n",
        "    class_weights = {0: (class1/total),\n",
        "                  1: ((class0/total))}\n",
        "\n",
        "    # Reset model weights\n",
        "    train_model.load_weights('original_model_weights.h5')\n",
        "\n",
        "    if finetuning_style == \"linear_probe\":\n",
        "      for layer in train_model.layers:\n",
        "        if layer.name == \"encoder_model\":\n",
        "            layer.trainable = False\n",
        "\n",
        "    if finetuning_style == \"full\":\n",
        "      for layer in train_model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    print(\" \")\n",
        "    # Verify by printing each layer's name and trainable status\n",
        "    for layer in train_model.layers:\n",
        "        print(layer.name, layer.trainable)\n",
        "    print(\" \")\n",
        "\n",
        "    # Train model\n",
        "    history = train_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs= 10000, # Edit\n",
        "        batch_size= 64,\n",
        "        validation_data = (X_val, y_val),\n",
        "        shuffle=False,\n",
        "        class_weight=class_weights,\n",
        "        callbacks = [early_stopper],\n",
        "        verbose = 2)\n",
        "\n",
        "    # Save model\n",
        "    current_model_name = f\"{ft_name}_n{size}_{finetuning_style}.h5\"\n",
        "    print(current_model_name)\n",
        "    train_model.save(root+current_model_name)\n",
        "\n",
        "    # Test model\n",
        "    test_scores = train_model.evaluate(X_test, y_test, batch_size=64) # Test Set\n",
        "    scores[\"test\"][finetuning_style][size] = test_scores[1]\n",
        "    print(\"Test AUC:\", test_scores[1])\n",
        "\n",
        "    val_scores = train_model.evaluate(X_val, y_val, batch_size=64) # Val Set\n",
        "\n",
        "    scores[\"val\"][finetuning_style][size] = val_scores[1]\n",
        "    print(\"Val AUC:\", val_scores[1])\n",
        "\n",
        "# Save all results in a .txt\n",
        "print(\"\\n\\n\")\n",
        "print(scores)\n",
        "\n",
        "results_path = f\"{results_root}{ft_name}_RESULTS.txt\"\n",
        "\n",
        "try:\n",
        "    file_to_write = open(results_path, 'wt')\n",
        "    file_to_write.write(str(scores))\n",
        "    file_to_write.close()\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "FLBWYOLfN7Vt"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}